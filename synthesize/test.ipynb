{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users/sb/anaconda3/envs/dif/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# 출력 디렉토리 설정\n",
    "output_dir = \"./test_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Stable Diffusion 관련 라이브러리 임포트\n",
    "from LocalStableDiffusionPipeline_Guide import  LocalStableDiffusionPipeline\n",
    "from diffusers import DPMSolverMultistepScheduler, UNet2DConditionModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(\n",
    "    pipeline,\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    image,\n",
    "    num_inference_steps=50,\n",
    "    strength=0.8,\n",
    "    guidance_scale=7.5,\n",
    "    #lambda_scale=None,  # CFG++에 사용\n",
    "    #cfg_type=\"cfg\",  # 'cfg' 또는 'cfg++'\n",
    "    output_dir=\"./test_results\",\n",
    "    file_prefix=\"test\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Stable Diffusion Pipeline에서 이미지를 생성하고 저장.\n",
    "\n",
    "    Args:\n",
    "        pipeline (StableDiffusionPipeline): 수정된 Stable Diffusion 파이프라인.\n",
    "        prompt (str): 텍스트 프롬프트.\n",
    "        negative_prompt (str): 부정적 텍스트 프롬프트.\n",
    "        image (PIL.Image): 초기 이미지.\n",
    "        num_inference_steps (int): 샘플링 스텝 수.\n",
    "        strength (float): 이미지에서 얼마나 강하게 변화를 줄 것인지 (0~1).\n",
    "        guidance_scale (float): CFG 강도.\n",
    "        lambda_scale (float): CFG++의 내삽 스케일 (CFG에서는 None 사용).\n",
    "        cfg_type (str): 'cfg' 또는 'cfg++' 선택.\n",
    "        output_dir (str): 생성된 이미지를 저장할 디렉토리.\n",
    "        file_prefix (str): 파일 이름 접두사.\n",
    "    \"\"\"\n",
    "    # 파이프라인에 CFG++ 설정 적용\n",
    "    #pipeline.scheduler.use_cfg_plus = (cfg_type == \"cfg++\")  # CFG++ 활성화 여부\n",
    "    #pipeline.scheduler.lambda_scale = lambda_scale if lambda_scale is not None else 0.5\n",
    "\n",
    "    # 이미지 생성\n",
    "    result = pipeline(\n",
    "        prompt=prompt,\n",
    "        image=image,\n",
    "        strength=strength,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        negative_prompt=negative_prompt,\n",
    "    )\n",
    "\n",
    "    # 결과 저장\n",
    "    result_image = result.images[0]  # 첫 번째 이미지를 저장\n",
    "    output_path = os.path.join(output_dir, f\"{file_prefix}_scale_{guidance_scale}.png\")\n",
    "    result_image.save(output_path)\n",
    "    print(f\"이미지 저장 완료: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 16.98it/s]\n",
      "You have disabled the safety checker for <class 'LocalStableDiffusionPipeline_Guide.LocalStableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "/data/users/sb/anaconda3/envs/dif/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py:325: FutureWarning: `_encode_prompt()` is deprecated and it will be removed in a future version. Use `encode_prompt()` instead. Also, be aware that the output format changed from a concatenated tensor to a tuple.\n",
      "  deprecate(\"_encode_prompt()\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "100%|██████████| 40/40 [00:03<00:00, 11.38it/s]\n",
      "/data/users/sb/anaconda3/envs/dif/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py:616: FutureWarning: The decode_latents method is deprecated and will be removed in 1.0.0. Please use VaeImageProcessor.postprocess(...) instead\n",
      "  deprecate(\"decode_latents\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 저장 완료: ./test_results/cityscape_scale_7.5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 저장 완료: ./test_results/cityscape_scale_0.7.png\n"
     ]
    }
   ],
   "source": [
    "# 수정된 Stable Diffusion 파이프라인 로드\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "scheduler = DPMSolverMultistepScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\").half()\n",
    "\n",
    "pipeline = LocalStableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    unet=unet,\n",
    "    scheduler=scheduler,\n",
    "    safety_checker=None,\n",
    "    #num_inference_steps=inference_step,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "# 테스트 프롬프트 및 초기 설정\n",
    "prompt = \"A futuristic cityscape at sunset, ultra-detailed, 4K\"\n",
    "negative_prompt = \"blurry, distorted, low resolution\"\n",
    "image = Image.open(\"init.JPEG\")  # 초기 이미지 경로\n",
    "num_inference_steps = 50\n",
    "strength = 0.8\n",
    "guidance_scale = 7.5\n",
    "\n",
    "# CFG 테스트\n",
    "generate_and_save_images(\n",
    "    pipeline=pipeline,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=image,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    strength=strength,\n",
    "    file_prefix=\"cityscape\"\n",
    ")\n",
    "\n",
    "# CFG++ 테스트\n",
    "generate_and_save_images(\n",
    "    pipeline=pipeline,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=image,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    strength=strength,\n",
    "    guidance_scale= 0.7,\n",
    "    file_prefix=\"cityscape\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# 이미지 로드\n",
    "cfg_image = Image.open(\"./test_results/cityscape_cfg.png\")\n",
    "cfg_plus_image = Image.open(\"./test_results/cityscape_cfg++.png\")\n",
    "\n",
    "# 결과 시각화\n",
    "print(\"CFG 결과:\")\n",
    "display(cfg_image)\n",
    "\n",
    "print(\"CFG++ 결과:\")\n",
    "display(cfg_plus_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possble solvers: ['dpm++_2m_cfg++']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This module includes LDM-based inverse problem solvers.\n",
    "Forward operators follow DPS and DDRM/DDNM.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Any, Callable, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from diffusers import DPMSolverMultistepScheduler,DDIMScheduler, StableDiffusionPipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "####### Factory #######\n",
    "__SOLVER__ = {}\n",
    "\n",
    "def register_solver(name: str):\n",
    "    def wrapper(cls):\n",
    "        if __SOLVER__.get(name, None) is not None:\n",
    "            raise ValueError(f\"Solver {name} already registered.\")\n",
    "        __SOLVER__[name] = cls\n",
    "        return cls\n",
    "    return wrapper\n",
    "\n",
    "def get_solver(name: str, **kwargs):\n",
    "    if name not in __SOLVER__:\n",
    "        raise ValueError(f\"Solver {name} does not exist.\")\n",
    "    return __SOLVER__[name](**kwargs)\n",
    "\n",
    "########################\n",
    "\n",
    "def get_ancestral_step(sigma_from, sigma_to, eta=1.):\n",
    "    \"\"\"Calculates the noise level (sigma_down) to step down to and the amount\n",
    "    of noise to add (sigma_up) when doing an ancestral sampling step.\"\"\"\n",
    "    if not eta:\n",
    "        return sigma_to, 0.\n",
    "    sigma_up = min(sigma_to, eta * (sigma_to ** 2 * (sigma_from ** 2 - sigma_to ** 2) / sigma_from ** 2) ** 0.5)\n",
    "    sigma_down = (sigma_to ** 2 - sigma_up ** 2) ** 0.5\n",
    "    return sigma_down, sigma_up\n",
    "\n",
    "\n",
    "def append_zero(x):\n",
    "    return torch.cat([x, x.new_zeros([1])])\n",
    "\n",
    "\n",
    "def get_sigmas_karras(n, sigma_min, sigma_max, rho=7., device='cpu'):\n",
    "    \"\"\"Constructs the noise schedule of Karras et al. (2022).\"\"\"\n",
    "    ramp = torch.linspace(0, 1, n+1, device=device)[:-1]\n",
    "    min_inv_rho = sigma_min ** (1 / rho)\n",
    "    max_inv_rho = sigma_max ** (1 / rho)\n",
    "    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\n",
    "    return append_zero(sigmas).to(device)\n",
    "\n",
    "########################\n",
    "\n",
    "class StableDiffusion():\n",
    "    def __init__(self,\n",
    "                 solver_config: Dict,\n",
    "                 model_key:str=\"runwayml/stable-diffusion-v1-5\",\n",
    "                 device: Optional[torch.device]='cuda',\n",
    "                 **kwargs):\n",
    "        self.device = device\n",
    "\n",
    "        self.dtype = kwargs.get(\"pipe_dtype\", torch.float16)\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_key, torch_dtype=self.dtype).to(device)\n",
    "        self.vae = pipe.vae\n",
    "        self.tokenizer = pipe.tokenizer\n",
    "        self.text_encoder = pipe.text_encoder\n",
    "        self.unet = pipe.unet\n",
    "\n",
    "        self.scheduler = DDIMScheduler.from_pretrained(model_key, subfolder=\"scheduler\")        \n",
    "        self.total_alphas = self.scheduler.alphas_cumprod.clone()\n",
    "        \n",
    "        self.sigmas = (1-self.total_alphas).sqrt() / self.total_alphas.sqrt()\n",
    "        self.log_sigmas = self.sigmas.log()\n",
    "        \n",
    "        total_timesteps = len(self.scheduler.timesteps)\n",
    "        self.scheduler.set_timesteps(solver_config.num_sampling, device=device)\n",
    "        self.skip = total_timesteps // solver_config.num_sampling\n",
    "\n",
    "        self.final_alpha_cumprod = self.scheduler.final_alpha_cumprod.to(device)\n",
    "        self.scheduler.alphas_cumprod = torch.cat([torch.tensor([1.0]), self.scheduler.alphas_cumprod])\n",
    "\n",
    "    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n",
    "        self.sample(*args, **kwargs)\n",
    "\n",
    "    def sample(self, *args: Any, **kwargs: Any) -> Any:\n",
    "        raise NotImplementedError(\"Solver must implement sample() method.\")\n",
    "\n",
    "    def alpha(self, t):\n",
    "        at = self.scheduler.alphas_cumprod[t] if t >= 0 else self.final_alpha_cumprod\n",
    "        return at\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_text_embed(self, null_prompt, prompt):\n",
    "        \"\"\"\n",
    "        Get text embedding.\n",
    "        args:\n",
    "            null_prompt (str): null text\n",
    "            prompt (str): guidance text\n",
    "        \"\"\"\n",
    "        # null text embedding (negation)\n",
    "        null_text_input = self.tokenizer(null_prompt,\n",
    "                                         padding='max_length',\n",
    "                                         max_length=self.tokenizer.model_max_length,\n",
    "                                         return_tensors=\"pt\",)\n",
    "        null_text_embed = self.text_encoder(null_text_input.input_ids.to(self.device))[0]\n",
    "\n",
    "        # text embedding (guidance)\n",
    "        text_input = self.tokenizer(prompt,\n",
    "                                    padding='max_length',\n",
    "                                    max_length=self.tokenizer.model_max_length,\n",
    "                                    return_tensors=\"pt\",\n",
    "                                    truncation=True)\n",
    "        text_embed = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
    "\n",
    "        return null_text_embed, text_embed\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        xt -> zt\n",
    "        \"\"\"\n",
    "        return self.vae.encode(x).latent_dist.sample() * 0.18215\n",
    "\n",
    "    def decode(self, zt):\n",
    "        \"\"\"\n",
    "        zt -> xt\n",
    "        \"\"\"\n",
    "        zt = 1/0.18215 * zt\n",
    "        img = self.vae.decode(zt).sample.float()\n",
    "        return img\n",
    "\n",
    "    def predict_noise(self,\n",
    "                      zt: torch.Tensor,\n",
    "                      t: torch.Tensor,\n",
    "                      uc: torch.Tensor,\n",
    "                      c: torch.Tensor):\n",
    "        \"\"\"\n",
    "        compuate epsilon_theta for null and condition\n",
    "        args:\n",
    "            zt (torch.Tensor): latent features\n",
    "            t (torch.Tensor): timestep\n",
    "            uc (torch.Tensor): null-text embedding\n",
    "            c (torch.Tensor): text embedding\n",
    "        \"\"\"\n",
    "        t_in = t.unsqueeze(0)\n",
    "        if uc is None:\n",
    "            noise_c = self.unet(zt, t_in, encoder_hidden_states=c)['sample']\n",
    "            noise_uc = noise_c\n",
    "        elif c is None:\n",
    "            noise_uc = self.unet(zt, t_in, encoder_hidden_states=uc)['sample']\n",
    "            noise_c = noise_uc\n",
    "        else:\n",
    "            c_embed = torch.cat([uc, c], dim=0)\n",
    "            z_in = torch.cat([zt] * 2)\n",
    "            t_in = torch.cat([t_in] * 2)\n",
    "            noise_pred = self.unet(z_in, t_in, encoder_hidden_states=c_embed)['sample']\n",
    "            noise_uc, noise_c = noise_pred.chunk(2)\n",
    "\n",
    "        return noise_uc, noise_c\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inversion(self,\n",
    "                  z0: torch.Tensor,\n",
    "                  uc: torch.Tensor,\n",
    "                  c: torch.Tensor,\n",
    "                  cfg_guidance: float=1.0):\n",
    "\n",
    "        # initialize z_0\n",
    "        zt = z0.clone().to(self.device)\n",
    "\n",
    "        # loop\n",
    "        pbar = tqdm(reversed(self.scheduler.timesteps), desc='DDIM Inversion')\n",
    "        for _, t in enumerate(pbar):\n",
    "            at = self.alpha(t)\n",
    "            at_prev = self.alpha(t - self.skip)\n",
    "\n",
    "            noise_uc, noise_c = self.predict_noise(zt, t, uc, c)\n",
    "            noise_pred = noise_uc + cfg_guidance * (noise_c - noise_uc)\n",
    "\n",
    "            z0t = (zt - (1-at_prev).sqrt() * noise_pred) / at_prev.sqrt()\n",
    "            zt = at.sqrt() * z0t + (1-at).sqrt() * noise_pred\n",
    "\n",
    "        return zt\n",
    "\n",
    "    def initialize_latent(self,\n",
    "                          method: str='random',\n",
    "                          src_img: Optional[torch.Tensor]=None,\n",
    "                          **kwargs):\n",
    "        if method == 'ddim':\n",
    "            z = self.inversion(self.encode(src_img.to(self.dtype).to(self.device)),\n",
    "                               kwargs.get('uc'),\n",
    "                               kwargs.get('c'),\n",
    "                               cfg_guidance=kwargs.get('cfg_guidance', 0.0))\n",
    "        elif method == 'npi':\n",
    "            z = self.inversion(self.encode(src_img.to(self.dtype).to(self.device)),\n",
    "                               kwargs.get('c'),\n",
    "                               kwargs.get('c'),\n",
    "                               cfg_guidance=1.0)\n",
    "        elif method == 'random':\n",
    "            size = kwargs.get('latent_dim', (1, 4, 64, 64))\n",
    "            z = torch.randn(size).to(self.device)\n",
    "        elif method == 'random_kdiffusion':\n",
    "            size = kwargs.get('latent_dim', (1, 4, 64, 64))\n",
    "            sigmas = kwargs.get('sigmas', [14.6146])\n",
    "            z = torch.randn(size).to(self.device)\n",
    "            z = z * (sigmas[0] ** 2 + 1) ** 0.5\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return z.requires_grad_()\n",
    "    \n",
    "    def timestep(self, sigma):\n",
    "        log_sigma = sigma.log()\n",
    "        dists = log_sigma.to(self.log_sigmas.device) - self.log_sigmas[:, None]\n",
    "        return dists.abs().argmin(dim=0).view(sigma.shape).to(sigma.device)\n",
    "\n",
    "    def to_d(self, x, sigma, denoised):\n",
    "        '''converts a denoiser output to a Karras ODE derivative'''\n",
    "        return (x - denoised) / sigma.item()\n",
    "    \n",
    "    def get_ancestral_step(self, sigma_from, sigma_to, eta=1.):\n",
    "        \"\"\"Calculates the noise level (sigma_down) to step down to and the amount\n",
    "        of noise to add (sigma_up) when doing an ancestral sampling step.\"\"\"\n",
    "        if not eta:\n",
    "            return sigma_to, 0.\n",
    "        sigma_up = min(sigma_to, eta * (sigma_to ** 2 * (sigma_from ** 2 - sigma_to ** 2) / sigma_from ** 2) ** 0.5)\n",
    "        sigma_down = (sigma_to ** 2 - sigma_up ** 2) ** 0.5\n",
    "        return sigma_down, sigma_up\n",
    "    \n",
    "    def calculate_input(self, x, sigma):\n",
    "        return x / (sigma ** 2 + 1) ** 0.5\n",
    "    \n",
    "    def calculate_denoised(self, x, model_pred, sigma):\n",
    "        return x - model_pred * sigma\n",
    "    \n",
    "    def kdiffusion_x_to_denoised(self, x, sigma, uc, c, cfg_guidance, t):\n",
    "        xc = self.calculate_input(x, sigma)\n",
    "        noise_uc, noise_c = self.predict_noise(xc, t, uc, c)\n",
    "        noise_pred = noise_uc + cfg_guidance * (noise_c - noise_uc)\n",
    "        denoised = self.calculate_denoised(x, noise_pred, sigma)\n",
    "        uncond_denoised = self.calculate_denoised(x, noise_uc, sigma)\n",
    "        return denoised, uncond_denoised\n",
    "\n",
    "\n",
    "\n",
    "@register_solver(\"dpm++_2m_cfg++\")\n",
    "class DPMpp2mCFGppSolver(StableDiffusion):\n",
    "    @torch.autocast(device_type='cuda', dtype=torch.float16)\n",
    "    def sample(self, cfg_guidance, prompt=[\"\", \"\"], callback_fn=None, **kwargs):\n",
    "        t_fn = lambda sigma: sigma.log().neg()\n",
    "        sigma_fn = lambda t: t.neg().exp()\n",
    "        # Text embedding\n",
    "        uc, c = self.get_text_embed(null_prompt=prompt[0], prompt=prompt[1])\n",
    "        # convert to karras sigma scheduler\n",
    "        total_sigmas = (1-self.total_alphas).sqrt() / self.total_alphas.sqrt()\n",
    "        sigmas = get_sigmas_karras(len(self.scheduler.timesteps), total_sigmas.min(), total_sigmas.max(), rho=7.)\n",
    "        # initialize\n",
    "        x = self.initialize_latent(method=\"random_kdiffusion\",\n",
    "                                   latent_dim=(1, 4, 64, 64),\n",
    "                                   sigmas=sigmas).to(torch.float16)\n",
    "        old_denoised = None # buffer\n",
    "        # Sampling\n",
    "        pbar = tqdm(self.scheduler.timesteps, desc=\"SD\")\n",
    "        for i, _ in enumerate(pbar):\n",
    "            sigma = sigmas[i]\n",
    "            new_t = self.timestep(sigma).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                denoised, uncond_denoised = self.kdiffusion_x_to_denoised(x, sigma, uc, c, cfg_guidance, new_t)\n",
    "\n",
    "            # solve ODE one step\n",
    "            t, t_next = t_fn(sigmas[i]), t_fn(sigmas[i+1])\n",
    "            h = t_next - t\n",
    "            if old_denoised is None or sigmas[i+1] == 0:\n",
    "                x = denoised + self.to_d(x, sigmas[i], uncond_denoised) * sigmas[i+1]\n",
    "            else:\n",
    "                h_last = t - t_fn(sigmas[i-1])\n",
    "                r = h_last / h\n",
    "                extra1 = -torch.exp(-h) * uncond_denoised - (-h).expm1() * (uncond_denoised - old_denoised) / (2*r)\n",
    "                extra2 = torch.exp(-h) * x\n",
    "                x = denoised + extra1 + extra2\n",
    "            old_denoised = uncond_denoised\n",
    "\n",
    "            if callback_fn is not None:\n",
    "                callback_kwargs = { 'z0t': denoised.detach(),\n",
    "                                    'zt': x.detach(),\n",
    "                                    'decode': self.decode}\n",
    "                callback_kwargs = callback_fn(i, new_t, callback_kwargs)\n",
    "                denoised = callback_kwargs[\"z0t\"]\n",
    "                x = callback_kwargs[\"zt\"]\n",
    "        \n",
    "        # for the last step, do not add noise\n",
    "        img = self.decode(x)\n",
    "        img = (img / 2 + 0.5).clamp(0, 1)\n",
    "        return img.detach().cpu()\n",
    "\n",
    "\n",
    "#############################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print all list of solvers\n",
    "    print(f\"Possble solvers: {[x for x in __SOLVER__.keys()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_solver(\n",
    "    solver,\n",
    "    cfg_guidance=7.5,\n",
    "    prompt=[\"A futuristic cityscape at sunset\", \"A futuristic cityscape at sunset\"],\n",
    "    num_inference_steps=50,\n",
    "    save_path=\"./cfg_plus_test_results/test_image.png\",\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    CFG++ Solver 테스트 함수\n",
    "    Args:\n",
    "        solver (StableDiffusion): DPMpp2mCFGppSolver 객체.\n",
    "        cfg_guidance (float): CFG 가이드 강도.\n",
    "        prompt (list): 텍스트 프롬프트. 첫 번째는 null prompt, 두 번째는 텍스트 조건.\n",
    "        num_inference_steps (int): 샘플링 스텝 수.\n",
    "        save_path (str): 결과 이미지를 저장할 경로.\n",
    "        verbose (bool): 진행 상황을 출력할지 여부.\n",
    "    \"\"\"\n",
    "    # Solver 초기화\n",
    "    solver.scheduler.set_timesteps(num_inference_steps, device=solver.device)\n",
    "    \n",
    "    # 이미지 생성\n",
    "    if verbose:\n",
    "        print(f\"CFG++ 테스트 시작 - Prompt: {prompt[1]}, CFG Scale: {cfg_guidance}\")\n",
    "    img = solver.sample(cfg_guidance=cfg_guidance, prompt=prompt)\n",
    "\n",
    "    # 이미지 저장\n",
    "    img_pil = ToPILImage()(img.squeeze(0))  # 텐서를 PIL 이미지로 변환\n",
    "    img_pil.save(save_path)\n",
    "    if verbose:\n",
    "        print(f\"결과 이미지 저장 완료: {save_path}\")\n",
    "\n",
    "    return img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG++ 테스트 시작 - Prompt: A futuristic cityscape at sunset, ultra-detailed, 4K, vibrant colors, CFG Scale: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SD: 100%|██████████| 50/50 [00:02<00:00, 17.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 이미지 저장 완료: ./cityscape_cfg_plus.png\n"
     ]
    }
   ],
   "source": [
    "# Solver 생성\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToPILImage\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class SolverConfig:\n",
    "    num_sampling: int\n",
    "    # 필요한 다른 매개변수를 추가 가능\n",
    "\n",
    "solver_config = SolverConfig(\n",
    "    num_sampling=50  # 샘플링 스텝 수\n",
    ")\n",
    "\n",
    "solver = get_solver('dpm++_2m_cfg++', solver_config=solver_config, model_key=\"runwayml/stable-diffusion-v1-5\")\n",
    "\n",
    "# 테스트 프롬프트 및 가이드 스케일\n",
    "prompt = [\n",
    "    \"\",  # Null prompt for unconditional guidance\n",
    "    \"A futuristic cityscape at sunset, ultra-detailed, 4K, vibrant colors\",\n",
    "]\n",
    "cfg_guidance = 0.6  # Classifier-Free Guidance scale\n",
    "\n",
    "# 결과 이미지 생성 및 저장\n",
    "output_path = os.path.join('./', \"cityscape_cfg_plus.png\")\n",
    "result_image = test_solver(\n",
    "    solver=solver,\n",
    "    cfg_guidance=cfg_guidance,\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=50,\n",
    "    save_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
